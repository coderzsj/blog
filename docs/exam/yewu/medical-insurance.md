---
title: 说说你对技术与业务的理解
category: 其他
tag: 
  - 业务工程
---

## 医保

北京新医保核心业务的开发(08.03~04.15)，

负责业务模块

1. 手工报销结算模块：
2. 票据扫描，票据文件报盘导入，提供两定接口，院端自费信息上传接口，
3. 院端上传的自费信息进行预结算，结算；添加相应的规则校验；
4. 手工报销结算模块(医照人员)模块：单位报盘自费信息导入，数据审核，补支、追回及其审核模块；对明细进行分割，各个分笔的费用进行金额的分割；
   - 预结算，将个人信息，就诊的信息，明细信息调支付方式，得出最后可以结算信息，(报销的金额，各项医保基金支出的金额)，如统筹基金，大额基金，军残基金；
   - 结算，将预结算算出的结算信息保存，同时保存参保人的待遇累计信息；
5. 手工报销费用补支、追回及其审核发财务；

单位将单位参保人自费的信息进行汇总，通过报盘导入，

金额是单位将明细的金额进行分类，保存的是分类项金额，不是具体的明细金额

灵活就业，纯手工录入七大项金额；就诊，明细信息需要机构传入，然后，再经办模块直接结算；模式增加了单位和机构的工作量；

补充校验规则，重复校验、转归死亡、跨年校验、是否已报销、定点校验、特病校验、门诊住院交叉校验

上传模块还可以进行批量结算，初审时，多次门诊交易可以结成一笔结算信息；

## 还做了一个批量导出的功能

项目前期给监管平台做的大数据批量导出的功能，前端查询模块查询业务数据，有导出功能，选择条件，将数据导出 excel 文件，每天业务量数据很大，每天都好几千万的数据。还要实现进度条的功能

1. 使用 easyexcel 框架，分批次导出，50 万作为一个 excel 的导出文件，假如有 1000 万条数据，需要 for 循环 20 次，每次循环，去用 limit 分批次查询，当所有 excel 文件生成，将 20 个 excel 文件打包，给前端提供压缩文件的地址
2. 使用流式查询，防止内存的溢出
3. 使用 redis 保存该用户各个导出文件的进度，前端专门画了一个页面去查看用户所有导出事件文件生成的进度，`redis` 结构使用 `hash` 结构，key 使用用户的 id，`value` 中的 `hash` 的 `key`，导出文件的文件名，value 是导出文件的生成的进度，当压缩文件生成完，`value` 为压缩文件的地址，前端使用定时器，不停的调查询接口，查询缓存中保存的进度

用户正在导出的数据过多，可以添加限制次数，前端可以判断 value 值是文件地址时，显示可导出，导出按钮将文件下载。

每次向文件里面追加数据，类似与 `aof`

调用 `ArrayList.clear()`，或者把对象置空,使用 `JProfile` 进行内存检测

通过 `JProfile` 内存分析.最终问题定位完毕.

原因如下：100W 数据  生成一个文件的过程中，等文件生成完毕之后才能把数据库中的数据备份到历史表中，这个时候才能进行事务的提交，也就是执行 commit()， 并且删除原表数据，100W 数据按照 3000 一批写入文件，每批次只是通过 `PreparedStatement.addBatch()`;加入到批次里面去,并没有执行 `PreparedStatement.executeBatch()`,而是在 `commit()` 之前统一调用的 `PreparedStatement.executeBatch()`，这样的话 `PreparedStatement` 就会缓存 100W 条数据信息，造成了内存溢出.

## 改进

`Spring Batch` 做批量。根据数据库的时间或者 ID 来进行批量处理后生成文件。然后将生成的文件放到某一个存储位置

## 审核不通过，逻辑修改

审核不通过时，对上传的信息做特殊处理，根据医院就诊 id，分组
